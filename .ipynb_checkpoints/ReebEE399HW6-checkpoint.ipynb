{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHRED applied to SST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook gives an introductory walkthrough to using SHRED models.  The dataset we consider is weekly mean sea-surface temperature as given by the NOAA Optimum Interpolation SST V2 dataset (https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.html).\n",
    "\n",
    "SHRED (SHallow REcurrent Decoder) models are a network architecture that merges a recurrent layer (LSTM) with a shallow decoder network (SDN) to reconstruct high-dimensional spatio-temporal fields from a trajectory of sensor measurements of the field. More formally, the SHRED architecture can be written as \n",
    "$$ \\mathcal {H} \\left( \\{ y_i \\} _{i=t-k}^t \\right) = \\mathcal {F} \\left( \\mathcal {G} \\left( \\{ y_i \\} _{i=t-k}^t \\right) ; W_{RN}) ; W_{SD} \\right)$$\n",
    "where $\\mathcal F$ is a feed forward network parameterized by weights $W_{SD}$, $\\mathcal G$ is a LSTM network parameterized by weights $W_{RN}$, and $\\{ y_i \\} _{i=t-k}^t$ is a trajectory of sensor measurements of a high-dimensional spatio-temporal field $\\{ x_i \\} _{i=t-k}^t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first randomly select 3 sensor locations and set the trajectory length (lags) to 52, corresponding to one year of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from processdata import load_data\n",
    "from processdata import TimeSeriesDataset\n",
    "import models\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_sensors = 3 \n",
    "lags = 52\n",
    "load_X = load_data('SST')\n",
    "n = load_X.shape[0]\n",
    "m = load_X.shape[1]\n",
    "sensor_locations = np.random.choice(m, size=num_sensors, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now select indices to divide the data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(n - lags, size=1000, replace=False)\n",
    "mask = np.ones(n - lags)\n",
    "mask[train_indices] = 0\n",
    "valid_test_indices = np.arange(0, n - lags)[np.where(mask!=0)[0]]\n",
    "valid_indices = valid_test_indices[::2]\n",
    "test_indices = valid_test_indices[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn's MinMaxScaler is used to preprocess the data for training and we generate input/output pairs for the training, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "sc = sc.fit(load_X[train_indices])\n",
    "transformed_X = sc.transform(load_X)\n",
    "\n",
    "### Generate input sequences to a SHRED model\n",
    "all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
    "for i in range(len(all_data_in)):\n",
    "    all_data_in[i] = transformed_X[i:i+lags, sensor_locations]\n",
    "\n",
    "### Generate training validation and test datasets both for reconstruction of states and forecasting sensors\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_data_in = torch.tensor(all_data_in[train_indices], dtype=torch.float32).to(device)\n",
    "valid_data_in = torch.tensor(all_data_in[valid_indices], dtype=torch.float32).to(device)\n",
    "test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
    "\n",
    "### -1 to have output be at the same time as final sensor measurements\n",
    "train_data_out = torch.tensor(transformed_X[train_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "valid_data_out = torch.tensor(transformed_X[valid_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data_in, train_data_out)\n",
    "valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)\n",
    "test_dataset = TimeSeriesDataset(test_data_in, test_data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "Error tensor(0.5041)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m shred \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mSHRED(num_sensors, m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, l1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m350\u001b[39m, l2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m validation_errors \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\EE 399\\RealGit\\pyshred\\models.py:90\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_dataset, valid_dataset, batch_size, num_epochs, lr, verbose, patience)\u001b[0m\n\u001b[0;32m     88\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     89\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, data[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 90\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "shred = models.SHRED(num_sensors, m, hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
    "validation_errors = models.fit(shred, train_dataset, valid_dataset, batch_size=64, num_epochs=200, lr=2*1e-3, verbose=True, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate reconstructions from the test set and print mean square error compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 1, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_recons \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39minverse_transform(\u001b[43mshred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      2\u001b[0m test_ground_truth \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39minverse_transform(test_dataset\u001b[38;5;241m.\u001b[39mY\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(test_recons \u001b[38;5;241m-\u001b[39m test_ground_truth) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(test_ground_truth))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\EE 399\\RealGit\\pyshred\\models.py:33\u001b[0m, in \u001b[0;36mSHRED.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m     h_0 \u001b[38;5;241m=\u001b[39m h_0\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     31\u001b[0m     c_0 \u001b[38;5;241m=\u001b[39m c_0\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m---> 33\u001b[0m _, (h_out, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m h_out \u001b[38;5;241m=\u001b[39m h_out[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m     36\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(h_out)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    726\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    729\u001b[0m                        ):\n\u001b[1;32m--> 730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    732\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    734\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    216\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 1, got 3"
     ]
    }
   ],
   "source": [
    "test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
    "test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
    "print(np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if \"Training epoch\" in line:\n",
    "            epoch = int(line.split()[-1])\n",
    "        elif \"Error\" in line:\n",
    "            error = float(line.split()[-1].replace(\"tensor(\", \"\").replace(\")\", \"\"))\n",
    "            data.append({'epoch': epoch, 'error': error})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = parse_file(\"3sens.txt\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df['epoch'], df['error'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training Error over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way to run multiple trainings of the model with Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to run one instance of the above code. Takes specified number of sensors and tags\n",
    "def run_experiment(num_sensors, lags, num_epochs):\n",
    "    num_sensors = num_sensors \n",
    "    lags = lags\n",
    "    load_X = load_data('SST')\n",
    "    n = load_X.shape[0]\n",
    "    m = load_X.shape[1]\n",
    "    sensor_locations = np.random.choice(m, size=num_sensors, replace=False)\n",
    "\n",
    "    train_indices = np.random.choice(n - lags, size=1000, replace=False)\n",
    "    mask = np.ones(n - lags)\n",
    "    mask[train_indices] = 0\n",
    "    valid_test_indices = np.arange(0, n - lags)[np.where(mask!=0)[0]]\n",
    "    valid_indices = valid_test_indices[::2]\n",
    "    test_indices = valid_test_indices[1::2]\n",
    "\n",
    "    sc = MinMaxScaler()\n",
    "    sc = sc.fit(load_X[train_indices])\n",
    "    transformed_X = sc.transform(load_X)\n",
    "\n",
    "    ### Generate input sequences to a SHRED model\n",
    "    all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
    "    for i in range(len(all_data_in)):\n",
    "        all_data_in[i] = transformed_X[i:i+lags, sensor_locations]\n",
    "\n",
    "    ### Generate training validation and test datasets both for reconstruction of states and forecasting sensors\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    train_data_in = torch.tensor(all_data_in[train_indices], dtype=torch.float32).to(device)\n",
    "    valid_data_in = torch.tensor(all_data_in[valid_indices], dtype=torch.float32).to(device)\n",
    "    test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
    "\n",
    "    ### -1 to have output be at the same time as final sensor measurements\n",
    "    train_data_out = torch.tensor(transformed_X[train_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "    valid_data_out = torch.tensor(transformed_X[valid_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "    test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(train_data_in, train_data_out)\n",
    "    valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)\n",
    "    test_dataset = TimeSeriesDataset(test_data_in, test_data_out)\n",
    "\n",
    "    shred = models.SHRED(num_sensors, m, hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
    "    validation_errors = models.fit(shred, train_dataset, valid_dataset, batch_size=64, num_epochs=num_epochs, lr=1e-3, verbose=True, patience=5)\n",
    "\n",
    "    test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
    "    test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
    "    print(np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth))\n",
    "    \n",
    "    return shred, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with 8 sensors...\n",
      "Training epoch 1\n",
      "Error tensor(0.4747)\n",
      "Training epoch 20\n",
      "Error tensor(0.2278)\n",
      "Training epoch 40\n",
      "Error tensor(0.2129)\n",
      "Training epoch 60\n",
      "Error tensor(0.2050)\n"
     ]
    }
   ],
   "source": [
    "num_sensors_range = range(8, 10)  # change this to the range you want\n",
    "lags = 52\n",
    "num_epochs = 200\n",
    "\n",
    "validation_errors_dict = {}\n",
    "test_performance_dict = {}\n",
    "\n",
    "for num_sensors in num_sensors_range:\n",
    "    print(f\"Running experiment with {num_sensors} sensors...\")\n",
    "    shred, validation_errors = run_experiment(num_sensors, lags, num_epochs)\n",
    "\n",
    "    # Compute test performance\n",
    "    test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
    "    test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
    "    test_performance = (np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth))\n",
    "\n",
    "    # Store validation errors and test performance\n",
    "    validation_errors_dict[num_sensors] = validation_errors\n",
    "    test_performance_dict[num_sensors] = test_performance\n",
    "    \n",
    "for num_sensors in num_sensors_range:\n",
    "    print(f\"Number of sensors: {num_sensors}\")\n",
    "    print(f\"Validation errors: {validation_errors_dict[num_sensors]}\")\n",
    "    print(f\"Test performance: {test_performance_dict[num_sensors]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "071f83251836d5bb3918d2af6501aef1a588d685a567aa45f470f25864dd9495"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
